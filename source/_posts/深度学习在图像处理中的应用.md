---
title: 深度学习在图像处理中的基础应用
date: 2021.11.3
author: Aik
img: ../images/deepLearning.jpg
top: true
hide: false
cover: true
coverImg: /images/1.jpg
password: 
toc: true
mathjax: true
summary: 包括图像分类、目标检测、图像分析分割等
categories: 深度学习
tags:
  - 图像处理
  - 深度学习
  - PyTorch
---

<!-- more -->

#  卷积神经网络基础

包括图像分类（AlexNet，VGGNet，GoogLeNet，ResNet）、目标检测以及图像分割等等，使用pytorch对其进行实现。参考视频：[卷积神经网络基础1](https://www.bilibili.com/video/BV1b7411T7DA?spm_id_from=333.999.0.0)    [卷积神经网络基础2](https://www.bilibili.com/video/BV1M7411M7D2/?spm_id_from=trigger_reload)

## 卷积神经网络

**CNN**(Convolutional Neural Network)：包含了卷积层的网络都可以叫做卷积神经网络

雏形：LeCun的LeNet（1998）网络结构

![image-20211211112307900](https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211211112307900.png)

从图中可以看出，这个网络从左往右包含了卷积层，下采样层，卷积层，下采样层，全连接层



## 全连接层

全连接层，由许许多多的神经元构成而得来的。

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201202751850.png" alt="image-20211201202751850" style="zoom: 33%;" />
将神经元按列进行排列，并且将列与列进行全连接，那么就能得到一个神经网络。
BP(back propagation)算法包括**信号的前向传播**和**误差的反向传播**两个过程。即计算误差输出时按从输入到输出的方向进行，而调整权值和阈值则从输出到输入的方向进行。

如下图的从左到右的正向传播过程中能够得到一个输出值，将这个输出值与我们期望的输出值进行对比，就能得到一个误差值

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201201450959.png" alt="image-20211201201450959" style="zoom: 33%;" />

通过计算每一个节点的偏导数，就能得到每个节点的误差梯度，然后我们将得到的损失值反向应用到损失梯度上，就达到了误差的反向传播过程。

下面通过一个实例来解释BP神经网络：

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201202024109.png" alt="image-20211201202024109" style="zoom:25%;" />

首先我们读入一张彩色的RGB图像，他的每个像素都含有三个值，就是三个RGB分量。

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201202344028.png" alt="image-20211201202344028" style="zoom:25%;" />

然后将它进行灰度化处理，进行灰度化之后，如下图所示，可以看到他的每个像素都只有一个值了。

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201203243414.png" alt="image-20211201203243414" style="zoom:25%;" />

最后进行二值化处理就得到了一个黑白图像，如下图

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201203503932.png" alt="image-20211201203503932" style="zoom:25%;" />

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201203555477.png" alt="image-20211201203555477" style="zoom:25%;" />

用五行三列的一个窗口在整个图像上进行滑动，每滑动一次就计算白色像素占整个像素的比例，在滑动到最右侧时要进行越界处理，最后能得到一个$5\times5$的矩阵。

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201204013150.png" alt="image-20211201204013150" style="zoom: 25%;" />

把这个$5\times5$的矩阵按行进行展开，拼接成一个行向量。这样就可以把这个行向量当作输入到神经网络的一个**输入层**。

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201204252390.png" alt="image-20211201204252390" style="zoom: 25%;" />

one-hot编码是常用的对标签进行编码的一种方式，可以保证最右侧一列每个输出的编码都不同， 这样就得到了一个**输出层**。

有了输入和期望的输出，就能对网络进行训练了。

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201204734587.png" alt="image-20211201204734587" style="zoom:33%;" />

将输入层节点数设为25个，输出层设为10个，中间的隐层按情况进行设置，这样就能对神经网络进行完整的训练了。

## 卷积层-CNN中独特的网络结构

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201210505827.png" alt="image-20211201210505827" style="zoom: 33%;" />

卷积就是**一个滑动窗口**在特征图上**滑动并进行计算**。如上图，以一个$3\times3$的卷积核为例，把卷积核覆盖到计算的特征层上面，也就是图中的橙色部分。将值分别进行相乘再相加，得到一个值，也就是粉色部分填写的值。这样每滑动一次能得到一个值，最后将$3\times3$的矩阵值填满也就得到了最终的卷积结果。

**卷积的目的是为了进行图像特征提取**

**卷积有两个特性**：

1. 拥有局部感知机制：因为滑动是局部的
2. 权值共享：因为在滑动计算过程中卷积核值是固定的

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201211158960.png" alt="image-20211201211158960" style="zoom: 33%;" />

**对于输入层而言，channel代表图像的通道数量，当输入图像为RGB彩色图像，则channel = 3；如果是灰度图像，则channel = 1。** 

**对于卷积核而言，卷积核的深度 = 卷积核channel数**

- 若当前卷积层的上一层为输入层，则channel数 = 输入图像channel数
- 无论输入图像的深度是多少，每经过一个卷积核都将变换为一个深度为1的特征图，一个卷积层之内可定义多个卷积核，当前卷积层上的各个卷积核会对上一层输入的每个feature map（特征图）分别执行卷积操作，即每个卷积核都会对应生成一个新的特征图feature map(不同的卷积核所提取的特征不同)。故而在下一层需要多少个特征图，本层就需要定义多少个卷积核，所以**卷积核的深度与传出的特征图的张数一致**。

普通的BP神经网络，假设如图中中间的隐藏层为1000个神经元，那么进行全连接时就会有921600000个参数生成。（**参数指连接层之间的权重参数**）

而卷积神经网络由于每个卷积核是固定的 ，一个卷积层所需要的参数就是25000个，这就体现了之前说的权值共享的特性。

前面所说的都是在一维的特征向量上进行卷积的，但实际过程中往往都是对多维的特征矩阵进行卷积操作。

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201212610650.png" alt="image-20211201212610650" style="zoom: 25%;" />

如上图，如果输入一张彩色的RGB图像，那么就会有R G B三个分量，那么**卷积核的深度也要和输入特征矩阵的深度保持一致**，也要是三维。将卷积核的每个维度放在对应的维度上进行滑动卷积计算，最后再进行求和操作就得到了一个卷积层。

红色卷积核在第一个位置计算结果为0，绿色卷积核在第一个位置计算结果为0，蓝色卷积核在第一个位置计算结果为1，$0+0+1=1$，那么输出特征矩阵的第一个位置就是1，全部计算完就能得到一个输出特征矩阵。如果我们在用一组卷积核2进行计算，那么有会得到一个输出特征矩阵。将这些输出特征矩阵进行拼接那么就能得到完整的输出特征矩阵。 

- 卷积核的深度（channel）与输入特征层的深度（channel）要相同：如图，输入的是三维的，那么卷积核也要是三维的。
- 输出的特征矩阵深度（channel）与卷积核个数相同：因为如图，每个卷积核都会生成一个输出特征矩阵。

**如果加上偏移量bias那么该怎么计算呢**

只需要与对应的输出特征矩阵相加运算。例如上面卷积核1的偏移量为-1，那么结果就是$$\begin{matrix}0&2\\0&0\end{matrix}$$,也就是在原本的结果上加上一个偏置项（偏移量）。卷积核2的偏移量为1，那么结果就是$$\begin{matrix}2&2\\3&2 \end{matrix}$$



**如果加上激活函数，那么又该如何进行计算呢？**

下面是两个常用的激活函数：

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201220705581.png" alt="image-20211201220705581" style="zoom: 33%;" />

首先要明白**为什么要使用激活函数**，是因为在我们的计算过程中这是一个线性的计算过程。引入非线性因素，使其具备解决非线性问题的能力，那么就需要通过一个非线性函数来达到这个目的。

现在常用的是Relu激活函数，因为，Sigmoid激活函数**达到饱和时梯度值变得非常小**，当值越来越大的时候，导数就基本**趋于0**了。所以当网络层数较深时容易出现**梯度消失**，在反向传播误差的过程中导数求起来也会很麻烦。

Relu函数的**导数就非常简单**，当$x$小于0的时候，导数就等于0，当$x>0$的时候，导数就是等于1。但它同时也有缺点，在反向传播过程中有一个**非常大的梯度经过**时，反向传播更新后可能会导致**权重分布中心小于0**，在进行正向传播时就可能会导致计算出的**结果是一个小于0的数**，如果小于0的话，那么经过Relu激活函数时就会**被过滤掉**，它的值就会始终等于0，这样它的反向传播就无法更新权重了。$x<0$的梯度都为0了，反向传播过程中就无法往前进行传播了，而且**权重进入失活**，并且失活后就无法再被激活的。所以在训练过程当中，不要一开始就使用一个特别大的学习率进行学习，这样很可能会导致很多神经元失活。



**如果卷积过程中出现越界时再怎么去处理？**

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201223322907.png" alt="image-20211201223322907" style="zoom: 33%;" />

如图出现越界问题时，一般通过padding的方式进行补0处理

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211201223529902.png" alt="image-20211201223529902" style="zoom:33%;" />

在卷积操作过程中，矩阵经卷积操作后的尺寸由以下几个因素决定：

- 输入图片大小$W \times W$：图中为$4\times4$的大小
- Filter（过滤器）的大小$F \times F$ ：图中为$3\times3$的大小
- 步长S：图中为2
- padding的像素数P： 正常是左右上下各补1个P，一共2P，图中只在右边和下边补了一个P

经卷积后的矩阵尺寸大小计算公式为：
$$
N=\frac{W-F+2P}{S}+1
$$
这里因为只补了1P，所以结果：
$$
N=\frac{W-F+P}{S}+1
 =\frac{4-3+1}{2}+1
 =2
$$
所以最终得到的特征矩阵的大小就为2.

## 池化层

**池化层的目的**是对特征图进行稀疏处理，减少数据运算量，从而降维（降低特征值的个数）。

池化层和卷积层比较类似，但是与卷积层相比较会简单许多。

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211202130739275.png" alt="image-20211202130739275" style="zoom: 33%;" />

如上图就是用一个$2\times2$的池化核进行**最大下采样**操作，将这个池化核放在左图的第一个位置上，然后找$\begin{matrix}1&1\\5&6 \end{matrix}$中的**最大值**，也就是6，于是右边输出的第一个位置就填6.

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211202131259074.png" alt="image-20211202131259074" style="zoom:33%;" />

如图是一个**平均下采样层**操作，与上面所说的最大下采样层最大的区别就是，拿第一个位置举例就是它是找$\begin{matrix}1&1\\5&6 \end{matrix}$这几个数的**平均值**，将平均值3.25作为输出。

**池化层**拥有以下几个**特点**：

- 没有训练参数

  卷积层的每个卷积核都会有自己的参数，像前面提到的RGB例子，每个卷积核都会有R G B三个参数。而池化层则没有参数，它只是在原本的特征层上求最大值或者平均数，也就不需要训练参数。

- 只改变特征矩阵的w和h，不改变深度（channel）

  就如上图所示是一个$4\times4$的特征层、深度为3，用一个$2\times2$、步距（stride）为2的池化核进行计算，那么最后得到的会是一个$2\times2$、深度为3的结果。

- 一般池化核大小（poolsize）和步距（stride）相同，并不是绝对的。

## 误差的计算

以下面一个三层的BP神经网络为例

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211204164844141.png" alt="image-20211204164844141" style="zoom:33%;" />

从左往右，第一层就是输入层，有两个节点：$x_1$和$x_2$。中间为隐层，有三个节点，其中以最上面的节点为例：

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211204165138340.png" alt="image-20211204165138340" style="zoom:33%;" />



最上面这个节点的输出就是$x_1$乘以它对应的权重$w_{11}$加上$x_2$乘以它对应的权重$w_{21}$，最后加上偏置$b_1$.通过激活函数$σ$就得到了隐层的第一个节点的输出。

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211204165831311.png" alt="image-20211204165831311" style="zoom: 33%;" />

同理如上图，可以求出$y_1$和$y_2$的输出，最后一层一般用的是softmax激活函数。

因为输出的$y_1$、$y_2$不属于任意一个分布，而我们想要它**满足概率分布**，所以要进行softmax处理。

计算表达式如下：
$$
o_{i}=\frac{e^{y_{i}}}{\sum_{j} e^{y_{j}}}
$$
根据表达式计算出来的结果如下：
$$
o_{1}=\frac{e^{y_{1}}}{e^{y_{1}}+e^{y_{2}}} \quad o_{2}=\frac{e^{y_{2}}}{e^{y_{1}}+e^{y_{2}}}
$$
经过softmax处理后的所有输出节点**概率和为1，**即$o_1+o_2=1$。

进行**交叉熵损失（Cross Entropy Loss）**的计算时，针对不同问题，会有不同种的计算方法。

1. 针对多分类问题（softmax输出，所有输出概率和为1）
   $$
   H=-{\sum}_io^*_ilog(o_i)
   $$
   大部分用的都是这一个，它的输出只会归为某一个类别。比如要么是猫要么是狗，不可能同时为多个。
   
2. 针对二分类问题（sigmoid输出，每个输出节点之间互不相干）
   $$
   H=-\frac{1}{N} \sum_{i=1}^{N}\left[o_{i}^{*} \log o_{i}+\left(1-o_{i}^{*}\right) \log \left(1-o_{i}\right)\right]
   $$

其中$o^*_i$为真实标签值，$o_i$为预测值，默认$log$以$e$为底等于$ln$

本例中是使用softmax激活函数，代入计算公式可以得到损失值：
$$
Loss=-(o_1^*log(o_1)+o_2^*log(o_2))
$$

## 误差的反向传播

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211208111742495.png" alt="image-20211208111742495" style="zoom: 33%;" />

以求$$w_{11}^{(2)}$$的误差梯度为例进行说明。它的$$损失值Loss=-(o_1^*log(o_1)+o_2^*log(o_2))$$,$$\begin{aligned}概率o_{1}=\frac{e^{y_{1}}}{e^{y_{1}}+e^{y_{2}}}\end{aligned}$$

$$
\begin{aligned} 输出y_{1}=w_{11}^{(2)} \cdot \sigma\left(x_{1} \cdot w_{11}^{(1)}+x_{2} \cdot w_{21}^{(1)}+b_{1}^{(1)}\right)+\\w_{21}^{(2)} \cdot \sigma\left(x_{1} \cdot w_{12}^{(1)}+x_{2} \cdot w_{22}^{(1)}+b_{2}^{(1)}\right)+\\w_{31}^{(2)} \cdot \sigma\left(x_{1} \cdot w_{13}^{(1)}+x_{2} \cdot w_{23}^{(1)}+b_{3}^{(1)}\right)+\\b_{1}^{(2)}\end{aligned}
$$

因为我们需要对$$w_{11}^{(2)}$$求损失梯度，那么上一个节点的输入$$\sigma\left(x_{1} \cdot w_{11}^{(1)}+x_{2} \cdot w_{21}^{(1)}+b_{1}^{(1)}\right)$$就可以看成一个常数进行处理，方便后续的计算。于是$$输出y_{1}=w_{11}^{(2)} \cdot \alpha_1+w_{21}^{(2)} \cdot \alpha_2+w_{31}^{(2)} \cdot \alpha_3+b_{1}^{(2)}$$.

求它的误差梯度也就是用损失值对它求偏导，于是根据链式求导法则：
$$
\begin{aligned}偏导数\frac{\partial \text { Loss }}{\partial w_{11}^{(2)}} =\frac{\partial L o s s}{\partial y_{1}} \cdot \frac{\partial y_{1}}{\partial w_{11}^{(2)}} =\left(\frac{\partial L o s s}{\partial o_{1}} \cdot \frac{\partial o_{1}}{\partial y_{1}}+\frac{\partial L o s s}{\partial o_{2}} \cdot \frac{\partial o_{2}}{\partial y_{1}}\right) \cdot \frac{\partial y_{1}}{\partial w_{11}^{(2)}}
\end{aligned}
$$

默认log代表以e为底

求解（1）式各个偏导数可得：
$$
\begin{align}
&\frac{\partial L o s s}{\partial o_{1}}=-o_{1}^{*} \cdot \frac{1}{o_{1}} \quad \frac{\partial L o s s}{\partial o_{2}}=-o_{2}^{*} \cdot \frac{1}{o_{2}}\\
&\frac{\partial o_{1}}{\partial y_{1}}=\frac{\partial\left(\frac{e^{y_{1}}}{e^{y_{1}}+e^{y_{2}}}\right)}{\partial y_{1}}=\frac{\partial\left(1-\frac{e^{y_{2}}}{e^{y_{1}}+e^{y_{2}}}\right)}{\partial y_{1}}=\frac{e^{y_{2}}}{\left(e^{y_{1}}+e^{y_{2}}\right)^{2}} \cdot e^{y_{1}}=o_{1} \cdot o_{2}=o_{1} \cdot\left(1-o_{1}\right)\\
&\frac{\partial o_{2}}{\partial y_{1}}=\frac{\partial\left(\frac{e^{y_{2}}}{e^{y_{1}}+e^{y_{2}}}\right)}{\partial y_{1}}=-\frac{e^{y_{2}}}{\left(e^{y_{1}}+e^{y_{2}}\right)^{2}} e^{y_{1}}=-o_{1} \cdot o_{2}=o_{1} \cdot\left(o_{1}-1\right) \\
&\frac{\partial y_{1}}{\partial w_{11}^{(2)}}=a_{1}
\end{align}
$$

将上述求得的结果代入到（1）式中去可以得到损失梯度：
$$
\begin{align}
\frac{\partial \text { Loss }}{\partial w_{11}^{(2)}} &=\frac{\partial \text { Loss }}{\partial y_{1}} \cdot \frac{\partial y_{1}}{\partial w_{11}^{(2)}} \\
&=\left(\frac{\partial \text { Loss }}{\partial o_{1}} \cdot \frac{\partial o_{1}}{\partial y_{1}}+\frac{\partial \text { Loss }}{\partial o_{2}} \cdot \frac{\partial o_{2}}{\partial y_{1}}\right) \cdot \frac{\partial y_{1}}{\partial w_{11}^{(2)}} \\
&=\left[-o_{1}^{*} \cdot \frac{1}{o_{1}} \cdot o_{1} \cdot o_{2}-o_{2}^{*} \cdot \frac{1}{o_{2}} \cdot\left(-o_{1} \cdot o_{2}\right)\right] \cdot a_{1} \\
&=\left(o_{2}^{*} \cdot o_{1}-o_{1}^{*} \cdot o_{2}\right) \cdot a_{1}
\end{align}
$$

## 权重的更新

还是对$$w_{11}^{(2)}$$用下面这一个公式进行权重的更新，进行反向的误差传播
$$
w_{11}^{(2)}(\text { new })=w_{11}^{(2)}(\text { old })-\text { learning }_{\text {rate }}(学习率) \cdot \text { gradient }(损失梯度)
$$

下面考虑这么一个问题，我们求得的损失梯度的方向是否是全局最优的方向

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211209171559117.png" alt="image-20211209171559117" style="zoom:33%;" />

如果使用整个样本集进行求解，损失梯度就会一直指向**全局最优方向**，如上图所示。但是在实际应用中往往不可能一次性将所有数据载入内存中，同时计算机的算力也不够，所以一般都会使用分批次（batch）的方法训练。

例如在使用ImageNet数据集的时候，它里面包含了1400万张的图像，不可能将所有的图像载入内存。而且如果想将这些图片全部计算完，以一个普通GPU的算力可能得算一个星期以上。如果非要得到它将所有的损失计算完之后在进行更新参数，首先，效率会很低，其次，如果中途计算机出现了问题，那么这一次就等于是白求了。所以一般都是使用分批次的方法，也就是比如一个数据集随机分成N等份，每份只取32张图片，也就是批次（batch）为32，每求完一个批次就进行一次误差计算以及误差的反向传播。

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211209171625708.png" alt="image-20211209171625708" style="zoom:33%;" />

如果使用分批次样本进行求解，损失梯度就指向**当前批次的最优方向**，如上图所示。就比如刚开始在第一个箭头处求了一批数据的损失，对那一批数据而言，它的最优方向就是箭头所指的方向，但下一次求了一批数据，它的最优方向又可能是另一个方向。与上面整个求解的方法比起来，训练过程不够平稳。

所以这里就要引出**优化器**（optimazer）的概念，优化器常见的有SGD、SGD+Momentun、Adagrad、RMSProp、RMSProp、Adam。

**优化器的目的**就是**使网络更快的得到收敛**，上面说的分批次求解就是**SGD优化器**（Stochastic Gradient Descent），SGD也就是我们常说的随机梯度下降。

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211209175738931.png" alt="image-20211209175738931" style="zoom: 50%;" />

### SGD优化器

表达式如下：
$$
w_{t+1}(更新后的参数)=w_t(更新前的参数)-\alpha(人工设置的学习率) \cdot g(w_t)(所求的t时刻对参数w_t的损失梯度)
$$
这个优化器存在一些**缺点**

- **易受样本噪声影响**：就比如训练集中有些样本的标注是错的，那么它所求出来的损失梯度就会有问题，说不定会与理想的所求梯度方向相背。

- **可能陷入局部最优解**：理想的情况是沿着黑色的梯度从上往下达到最优解，但也有可能如下图沿着红色箭头方向到达一个局部最优解并陷入到里面去了。

  <img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211209181158088.png" alt="image-20211209181158088" style="zoom: 50%;" />

为了解决这个问题，我们就会采用SGD+Momentum（动量）来进行优化。

### SGD+Momentum优化器

表达式如下：
$$
\begin{align}
v_t&=\eta(动量系数，一般取0.9) \cdot v_{t-1}+\alpha(学习率)\cdot g(w_t)(所求的t时刻对参数w_t的损失梯度) \\
w_{t+1}&=w_t-v_t
\end{align}
$$
<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211209184941790.png" alt="image-20211209184941790" style="zoom:33%;" />

例如这一次求的梯度方向是$$g_{t+1}$$,上一次求得梯度方向是$$g_{(t)}$$,那么如果引入动量之后，在求梯度方向时它就会考虑上一次的梯度方向$$g_{(t)}$$，最终这一次的梯度方向可能就是$$g^*_{t+1}$$.这样就能有效的抑制样本噪声的干扰。



### Adagrad优化器（自适应学习率）

表达式如下：
$$
\begin{align}
s_{t}(当前s)&=s_{t-1}(上一次的s)+g\left(w_{t}\right) \cdot g\left(w_{t}\right)(损失梯度的平方) \\
w_{t+1}(最终更新的参数)&=w_{t}(当前参数)-\frac{\alpha}{\sqrt{s_{t}+\varepsilon}}(学习率) \cdot g(w_t)(损失梯度)
\end{align}
$$
从名字就可以看出它是在学习率上动了手脚。 可以从式子中发现，由于$$s_t$$是越来越大的，导致学习率在训练过程中会越来越小。但它也有一个问题，就是**学习率下降的太快**，可能还没达到收敛训练就停止了。下面的RMSProp优化器就是解决这么一个问题而出现的。

### RMSProp优化器（自适应学习率）

表达式如下：
$$
\begin{align}
s_{t}&=\eta \cdot s_{t-1}+(1-\eta) \cdot g\left(w_{t}\right) \cdot g\left(w_{t}\right) \\
w_{t+1}&=w_{t}-\frac{\alpha}{\sqrt{s_{t}+\varepsilon}} \cdot g\left(w_{t}\right)
\end{align}
$$
相比上面的Adagrad优化器，它就多了两个系数，$$\eta$$和$$(1-\eta)$$,用来控制学习率的衰减速度。

### Adam优化器（自适应学习率）

表达式如下：
$$
\begin{align}
m_{t}&=\beta_{1} \cdot m_{t-1}+\left(1-\beta_{1}\right) \cdot g\left(w_{t}\right)\hspace{1em} 一阶动量\\
v_{t}&=\beta_{2} \cdot v_{t-1}+\left(1-\beta_{2}\right) \cdot g\left(w_{t}\right) \cdot g\left(w_{t}\right)\hspace{1em}二阶动量  \\
\hat{m}_{t}&=\frac{m_{t}}{1-\beta_{1}^{t}} \hat{v}_{t}=\frac{v_{t}}{1-\beta_{2}^{t}} \\
w_{t+1}&=w_{t}-\frac{\alpha}{\sqrt{\hat{v}_{t}+\varepsilon}} \hat{m}_{t}
\end{align}
$$
相比前面几种优化器要复杂一些，它包含了一阶动量和二阶动量.



![](https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/SGD.gif)

上图展示了一系列优化器优化速度的快慢，可以看到SGD虽然最慢，但是它的方向是沿着比较理想的方向更新的；Momentun刚开始有点偏，但很快就找回了正确路径；Adagrad和Rmsprop不仅方向正确并且速度也很快。

日常使用中，常见的还是SGD+Momentum以及Adam优化器，尤其是前者用的比较广泛。SGD虽然可能会慢点，但可能最终达到的效果是最优的。

# 分类网络

## Lenet

[参考视频](https://www.bilibili.com/video/BV187411T7Ye?spm_id_from=333.999.0.0)

首先配置好Anaconda环境，PyCharm，然后下载[PyTorch](https://pytorch.org/get-started/locally/)

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211211110614784.png" alt="image-20211211110614784" style="zoom: 67%;" />

按如图选择适合的版本，CUDA版本从NVIDIA控制面板的帮助中查看，在帮助-系统信息-组件中可以看到适配的CUDA版本，如果没有N卡独显那么就直接选择CPU版本。

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211211110858309.png" alt="image-20211211110858309" style="zoom: 67%;" />

直接从PyCharm的Terminal栏将命令复制进去执行进行安装。

![image-20211211112307900](https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211211112307900.png)

**LeNet**在这个网络中使用的是灰度图像，他的输入为$$32\times32$$的灰度图像，只有一个维度，深度为1。

但我们下面例子中使用的CIFAR10数据集是采用的彩色的图像，所以它的输入会有R G B三个通道，所以输入的为$$3\times32\times32$$这么一个大小。

**Pytorch Tensor**的**通道排序**为：[batch,channel,height,width]

*Tensor可以理解成一个多维数组，目的是能够创造更高维度的矩阵、向量*

- batch就是一批图像的个数，比如我们每批输入32张图片
- 由于CIFAR10数据集的图片为彩色，所以channel就为3
-  height和width这里都是32

后面在对Tendor进行处理的时候都是根据这个顺序进行换算的。

以上图为例进行**LeNet**模型的搭建：

**model.py**

```python
import torch.nn as nn
import torch.nn.functional as F

class LeNet(nn.Module):     # 定义一个类继承自nn.Module父类
    def __init__(self):     # 实现初始化函数，包括搭建过程中使用到的网络层结构参数
        super(LeNet, self).__init__()       # super()继承父类的构造函数
        self.conv1 = nn.Conv2d(3, 16, 5)    # 定义一个卷积层 channel：3,使用卷积核个数：16，卷积核大小：5
        self.pool1 = nn.MaxPool2d(2, 2)     # 定义一个下采样层 池化核大小：2，步长：2
        self.conv2 = nn.Conv2d(16, 32, 5)    # 定义第二个卷积层，因为上一个输出的深度为16，所以这里要把深度也置为16
        self.pool2 = nn.MaxPool2d(2, 2)     # 定义第二个下采样层
        self.fc1 = nn.Linear(32*5*5, 120)       # 全连接层 要把节点全部展平
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)        # 使用的数据集为10个类别的，所以设为10

    def forward(self, x):       # 在这个函数中定义正向传播的过程
        x = F.relu(self.conv1(x))    # input(3, 32, 32) output(16, 28, 28)  relu为激活函数
        x = self.pool1(x)            # output(16, 14, 14)
        x = F.relu(self.conv2(x))    # output(32, 10, 10)
        x = self.pool2(x)            # output(32, 5, 5)
        x = x.view(-1, 32*5*5)       # output(32*5*5)   使用view函数展成一维向量
        x = F.relu(self.fc1(x))      # output(120)
        x = F.relu(self.fc2(x))      # output(84)
        x = self.fc3(x)              # output(10)   在训练网络过程中，卷积交叉熵的过程中已经在内部实现了一个更加高效的softmax方法。所以这边最后一层不需要再添加softmax层了
        return x

# 调试建立的模型
# import torch
# #随机生成一个输入，banch=32，channel=3，高度=32，宽度=32
# input1 = torch.rand([32, 3, 32, 32])
# model = LeNet()     # 实例化模型
# print(model)
# #将数据输入到网络中进行正向传播
# output = model(input1)
```

进行模型的训练（CPU）：

**train.py**

```python
import matplotlib.pyplot as plt
import numpy as np
import torch
import torchvision
import torch.nn as nn
from model import LeNet
import torch.optim as optim
import torchvision.transforms as transforms
#
#
# def main():
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
#
# 50000张训练图片
# 第一次使用时要将download设置为True才会自动去下载数据集
# train为True就会导入训练集的样本，transform函数对图像进行预处理
train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
# 将训练集分成很多个批次。batch_size=36,每一批拿出36张图片进行训练；shuffle表示是否将数据集中的数据随机提取出来，num_workers表示线程数，windows只能设为0
train_loader = torch.utils.data.DataLoader(train_set, batch_size=36, shuffle=True, num_workers=0)

# 10000张验证图片
# 第一次使用时要将download设置为True才会自动去下载数据集
test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform)
test_loader = torch.utils.data.DataLoader(test_set, batch_size=10000, shuffle=False, num_workers=0)
# 将测试参数转换为可迭代的迭代器，通过next函数就能获得一批图像，包括图像以及他的标签值
val_data_iter = iter(test_loader)
val_image, val_label = val_data_iter.next()

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')     # 数据集对应的所有类别

# 查看测试数据结果
# def imshow(img):
#     img = img / 2 + 0.5     # unnormalize
#     npimg = img.numpy()     # 转numpy格式的img
#     plt.imshow(np.transpose(npimg, (1, 2, 0)))
#     plt.show()
#
# # print labels
# print(' '.join('%5s' % classes[val_label[j]] for j in range(4)))
# # show images
# imshow(torchvision.utils.make_grid(val_image))

net = LeNet()       # 实例化前面定义的模型
loss_function = nn.CrossEntropyLoss()       # 定义损失函数，这里就已经包含了softmax函数了，所以后面不需要再定义
optimizer = optim.Adam(net.parameters(), lr=0.001)      # 定义优化器，net所有参数都进行训练，学习率定位0.001

for epoch in range(5):  # loop over the dataset multiple times  训练集迭代次数

    running_loss = 0.0  # 通过变量来累加训练过程中的损失
    for step, data in enumerate(train_loader, start=0):     # 使用enumerate函数，不仅会返回每一批的data，还对返回相应的索引
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data   # 将数据分离成图像和标签

        # zero the parameter gradients
        optimizer.zero_grad()   # 将历史损失梯度清零，可以一次性计算多个小batch从而得到一个大的batch，来弥补硬件不足以一次训练很大的batch，从而得到更好的结果。
        # forward + backward + optimize
        outputs = net(inputs)   # 图片输入到网络进行正向传播
        loss = loss_function(outputs, labels)    # 通过损失函数计算损失 outputs=网络得到的预测值，labels=图片对应标签
        loss.backward()     # 将损失反向传播
        optimizer.step()    # 进行参数的更新

        # print statistics
        running_loss += loss.item()
        if step % 500 == 499:    # print every 500 mini-batches
            with torch.no_grad():   # 这个函数内不会进行误差梯度的计算
                outputs = net(val_image)  # [batch, 10] 正向传播
                predict_y = torch.max(outputs, dim=1)[1]    # 查找输出的最大的index在什么位置，概率最高的在哪，也就是网络预测最可能是哪个类别的
                accuracy = torch.eq(predict_y, val_label).sum().item() / val_label.size(0)  # 将预测的标签类别与真实的比较，对的会返回true，求和除以总样本得准确率

                print('[%d, %5d] train_loss: %.3f  test_accuracy: %.3f' %
                      (epoch + 1, step + 1, running_loss / 500, accuracy))      # epoch=训练到第几轮，step=某一轮的多少步，running_loss / 500=平均训练误差，accuracy=测试样本的准确率
                running_loss = 0.0      # 清零进行下一次500迭代

print('Finished Training')
#
save_path = './Lenet.pth'
torch.save(net.state_dict(), save_path)     # 模型权重参数进行保存
#
#
# if __name__ == '__main__':
#     main()
```

进行模型的训练（GPU）：

**train.py**

```PYTHON
import matplotlib.pyplot as plt
import numpy as np
import torch
import torchvision
import torch.nn as nn
from model import LeNet
import torch.optim as optim
import torchvision.transforms as transforms
#
#
# def main():
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
#
# 50000张训练图片
# 第一次使用时要将download设置为True才会自动去下载数据集
# train为True就会导入训练集的样本，transform函数对图像进行预处理
train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
# 将训练集分成很多个批次。batch_size=36,每一批拿出36张图片进行训练；shuffle表示是否将数据集中的数据随机提取出来，num_workers表示线程数，windows只能设为0
train_loader = torch.utils.data.DataLoader(train_set, batch_size=36, shuffle=True, num_workers=0)

# 10000张验证图片
# 第一次使用时要将download设置为True才会自动去下载数据集
test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform)
test_loader = torch.utils.data.DataLoader(test_set, batch_size=10000, shuffle=False, num_workers=0)
# 将测试参数转换为可迭代的迭代器，通过next函数就能获得一批图像，包括图像以及他的标签值
val_data_iter = iter(test_loader)
val_image, val_label = val_data_iter.next()

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')     # 数据集对应的所有类别

# 查看测试数据结果
# def imshow(img):
#     img = img / 2 + 0.5     # unnormalize
#     npimg = img.numpy()     # 转numpy格式的img
#     plt.imshow(np.transpose(npimg, (1, 2, 0)))
#     plt.show()
#
# # print labels
# print(' '.join('%5s' % classes[val_label[j]] for j in range(4)))
# # show images
# imshow(torchvision.utils.make_grid(val_image))
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)
net = LeNet()       # 实例化前面定义的模型
net.to(device)
loss_function = nn.CrossEntropyLoss()       # 定义损失函数，这里就已经包含了softmax函数了，所以后面不需要再定义
optimizer = optim.Adam(net.parameters(), lr=0.001)      # 定义优化器，net所有参数都进行训练，学习率定位0.001

for epoch in range(5):  # loop over the dataset multiple times  训练集迭代次数

    running_loss = 0.0  # 通过变量来累加训练过程中的损失
    for step, data in enumerate(train_loader, start=0):     # 使用enumerate函数，不仅会返回每一批的data，还对返回相应的索引
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data   # 将数据分离成图像和标签

        # zero the parameter gradients
        optimizer.zero_grad()   # 将历史损失梯度清零，可以一次性计算多个小batch从而得到一个大的batch，来弥补硬件不足以一次训练很大的batch，从而得到更好的结果。
        # forward + backward + optimize
        outputs = net(inputs.to(device))   # 图片输入到网络进行正向传播
        loss = loss_function(outputs, labels.to(device))    # 通过损失函数计算损失 outputs=网络得到的预测值，labels=图片对应标签
        loss.backward()     # 将损失反向传播
        optimizer.step()    # 进行参数的更新

        # print statistics
        running_loss += loss.item()
        if step % 500 == 499:    # print every 500 mini-batches
            with torch.no_grad():   # 这个函数内不会进行误差梯度的计算
                outputs = net(val_image.to(device))  # [batch, 10] 正向传播
                predict_y = torch.max(outputs, dim=1)[1]    # 查找输出的最大的index在什么位置，概率最高的在哪，也就是网络预测最可能是哪个类别的
                accuracy = torch.eq(predict_y, val_label.to(device)).sum().item() / val_label.size(0)  # 将预测的标签类别与真实的比较，对的会返回true，求和除以总样本得准确率

                print('[%d, %5d] train_loss: %.3f  test_accuracy: %.3f' %
                      (epoch + 1, step + 1, running_loss / 500, accuracy))      # epoch=训练到第几轮，step=某一轮的多少步，running_loss / 500=平均训练误差，accuracy=测试样本的准确率
                running_loss = 0.0      # 清零进行下一次500迭代

print('Finished Training')
#
save_path = './Lenet.pth'
torch.save(net.state_dict(), save_path)     # 模型权重参数进行保存
#
#
# if __name__ == '__main__':
#     main()
```

训练结果如下：

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211222213759995.png" alt="image-20211222213759995" style="zoom: 50%;" />

基于训练好的模型进行预测：

**predict.py**

```python
import torch
import torchvision.transforms as transforms
from PIL import Image

from model import LeNet


def main():
    transform = transforms.Compose(
        [transforms.Resize((32, 32)),
         transforms.ToTensor(),
         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

    classes = ('plane', 'car', 'bird', 'cat',
               'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

    net = LeNet()
    net.load_state_dict(torch.load('Lenet.pth'))    # 载入保存的权重参数文件

    im = Image.open('1.jpg')       # 载入预测图片
    im = transform(im)  # [C, H, W]     # 预处理后就得到了深度，高度，宽度三个维度
    im = torch.unsqueeze(im, dim=0)  # [N, C, H, W]     # 因为pytorch所要求的要有batch，所以在最前面加一个新的维度

    with torch.no_grad():
        outputs = net(im)
        predict = torch.max(outputs, dim=1)[1].data.numpy()
        # predict = torch.softmax(outputs, dim=1)
    print(classes[int(predict)])
    # print(predict)


if __name__ == '__main__':
    main()
```

加载放在目录下的1.jpg，得到结果

<img src="https://hexo0.oss-cn-shanghai.aliyuncs.com/blog/img/image-20211222214035215.png" alt="image-20211222214035215" style="zoom:50%;" />

## AlexNet网络结构

[参考视频](https://www.bilibili.com/video/BV1p7411T7Pc?spm_id_from=333.999.0.0)

![image-20220316150242394](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220316150242394.png)

该网络把分类准确率由传统的70%提升到80%。拥有以下几个特点：

1. 首次利用GPU进行网络加速训练
2. 使用了ReLU激活函数，而不是传统的Sigmoid激活函数以及Tanh激活函数
3. 使用了LRN局部响应归一化
4. 在全连接层的前两层中使用了Dropout随机失活神经元操作，以减少过拟合

![image-20220316150654168](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220316150654168.png)

过拟合会导致泛化能力很差，导致对新样本的测试数据很差。

使用**Dropout**的方式在网络正向传播过程中**随机失活**一部分神经元，减少传播过程中的参数从而达到减少过拟合的一个目的。

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220316150837828.png" alt="image-20220316150837828" style="zoom: 33%;" />

![image-20220316151228147](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220316151228147.png)

![image-20220316151302229](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220316151302229.png)

![image-20220316151356073](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220316151356073.png)

![image-20220316151447703](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220316151447703.png)

![image-20220316151510025](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220316151510025.png)

![image-20220316151530718](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220316151530718.png)

![image-20220316151559332](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220316151559332.png)

![image-20220316151631964](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220316151631964.png)

最后进入全连接层，1000是类别数目

## VGG网络结构及感受野的计算

[参考视频](https://www.bilibili.com/video/BV1q7411T7Y6/?spm_id_from=333.788)

![image-20220316171844216](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220316171844216.png)

该网络的特点：通过堆叠多个$$3\times3$$的卷积核来替代大尺度卷积核（减少所需参数）

**感受野：**

在卷积神经网络中，决定某一层输出结果中一个元素所对应的输入层的区域大小，被称作感受野。

通俗解释就是输出特征层上的一个单元对应输入层上的区域大小。

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220316170727310.png" alt="image-20220316170727310" style="zoom: 33%;" />

![image-20220316170804803](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220316170804803.png)

VGG的论文中提到，可以通过堆叠两个$$3\times3$$的卷积核替代$$5\times5$$的卷积核，堆叠三个$$3\times3$$的卷积核替代$$7\times7$$的卷积核。

使用$$7\times7$$卷积核所需参数，与堆叠三个$$3\times3$$卷积核所需参数进行对比，假设输入输出channel为C
$$
\begin{align}&7\times7\times C\times C=49C^2 \\
&3\times3\times C\times C+3\times3\times C\times C+3\times3\times C\times C=27C^2
\end{align}
$$
可以看到参数明显减少。



使用5类共3000张图片训练得到的结果如下：

![image-20220324192632243](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324192632243.png)

## GoogLeNet

该网络特点：

1. 引入了Inception结构（融合不同尺度的特征信息）
2. 使用1x1的卷积核进行降维以及映射处理
3. 添加两个辅助分类器帮助训练
4. 丢弃全连接层，使用平均池化层（大大减少模型参数）

 **Inception结构**

![image-20220325125652863](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220325125652863.png)

如图a，采用并联的方式，将上一层输出的特征矩阵同时输入到4个分支进行处理，将处理过后的特征矩阵按深度进行拼接得到输出特征矩阵。

图b，多了三个1x1的卷积层来起到降维的作用，减少了特征矩阵的深度，减少卷积参数从而减少计算量

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220325130209727.png" alt="image-20220325130209727" style="zoom: 25%;" />

**辅助分类器**

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220325130451456.png" alt="image-20220325130451456" style="zoom: 33%;" />

网络中采用两个相同的辅助分类器，防止反向传播过程中产生的梯度爆炸和梯度消失问题



## ResNet

[参考视频](https://www.bilibili.com/video/BV1T7411T7wa/?spm_id_from=333.788)

网络结构：

![image-20220325132019215](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220325132019215.png)

网络特点：

1. 超深的网络结构
2. 提出residual模块
3. 使用Batch Normalization加速训练（丢弃dropout）

能够解决反向传播过程中的梯度消失或者梯度爆炸的问题，以及退化问题。

![image-20220325131224873](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220325131224873.png)

简单的通过堆叠网络深度并不一定能得到更好的效果

假设每一层的误差梯度是一个小于1的数，那么在反向传播过程中，每向前传播一层，都要乘以一个小于1的误差梯度，当网络越来越深的时候，乘以这个数的次数也就越来越多导致梯度越来越趋近0，假设每一层的误差梯度是一个大于1的数，那么最终也会导致梯度爆炸的现象

为了解决这个问题，一般都会采用数据标准化处理，权重初始化的方式，以及该网络中所提到的BN（batch normalization）

解决了梯度消失以及梯度爆炸的问题以后，还是会面临退化问题，通过残差结构就可以解决这个问题

![image-20220323170732279](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220323170732279.png)

**残差结构**，将输入特征矩阵与输出特征矩阵进行相加，得到最终的输出，channel相同就可以直接加，如上图左；不相同则通过对维度进行调整得到相同维度再相加，如上图右。

<img src="https://img-blog.csdnimg.cn/20181111233018131.png" alt="img" style="zoom: 80%;" />

如上图ResNet-34就大量采用了这种方式进行网络层数的堆叠。



**Batch Normalization**的目的是使我们的一批（Batch）feature map满足均值为0，方差为1的分布规律。

![image-20220323171917724](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220323171917724.png)

## 迁移学习

使用迁移学习的**优势**：

1. 能够快速的训练出一个理想的结果
2. 当数据集较小时也能训练出理想的效果

注意：使用别人预训练模型参数时，要注意别人的预处理方式。

![image-20220323172516588](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220323172516588.png)

新的网络能够直接使用别人训练好的底层通用特征的浅层网络参数，这样就能更快的学习新的数据集的高维特征 

**常见的迁移学习方式：**

1. 载入权重后训练所有参数
2. 载入权重后只训练最后几层参数
3. 载入权重后在原网络基础上再添加一层全连接层，仅训练最后一个全连接层



不使用迁移学习进行ResNet网络的训练结果：

![image-20220324191820739](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324191820739.png)

30轮迭代过后准确率最高能够达到0.857

使用官方在imagenet数据集上训练得到的权重使用迁移学习的方式进行训练：

![image-20220324192334527](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324192334527.png)

可以看到仅3轮迭代就可以达到0.92的准确率

## ResNeXt

 相比ResNet 引入组卷积的概念，使得这下面三个block模块完全等价

![image-20220323222936382](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220323222936382.png)

![image-20220323222755735](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220323222755735.png)

![image-20220323222915773](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220323222915773.png)

下图为ResNet-50和ResNeXt-50的参数对比

![image-20220323223110380](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220323223110380.png)



# 目标检测

## R-CNN

R-CNN框架：

![image-20220324205830715](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324205830715.png)

RCNN算法流程可分成4个步骤：

1. 一张图像生成1000~2000个**候选区域**（使用Selective Search方法）
2. 对每个候选区域，使用深度网络**提取特征**
3. 特征送入每一类的**SVM分类器**，判断是否属于该类
4. 使用回归器**精细修正**候选框位置

### 正向传播过程

- 候选区域的生成

  利用Selective Search算法用过图像分割的方法得到一些原始区域，然后使用一些合并策略将这些区域合并，得到一个层次化的区域结构，而这些结构就包含着可能需要的物体。



<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324193125955.png" alt="image-20220324193125955" style="zoom: 25%;" />

- 对每个候选区域，使用深度网络提取特征

  将2000候选区域缩放到$$227\times227$$pixel，接着将候选区域输入事先训练好的AlexNet CNN网络获取4096维的特征得到$$2000\times4096$$维矩阵。与之前分类网络不同的点在于最后不需要对向量进行展平处理再输入到全连接层中，而是保持特征向量的形式。 

![image-20220324193444277](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324193444277.png)

- 特征送入每一类的SVM分类器，判定类别

  将$$2000\times4096$$维特征与20个SVM组成的权值矩阵$$4096\times20$$相乘，获得$$2000\times20$$维矩阵标识每个建议框是某个目标类别的得分。分别对上述$$2000\times20$$维矩阵中每一列即每一类进行**非极大值抑制**剔除重叠建议框，得到该列即该类中得分最高的一些建议框

  <img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324194131781.png" alt="image-20220324194131781" style="zoom:33%;" />

  <img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324194319201.png" alt="image-20220324194319201" style="zoom:33%;" />

  例如左边第一行为第一个候选框的特征向量，右边20列分别是20类的SVM分类器，与第一列相乘，假设这是判别为猫的分类器，那么得到右边**概率矩阵**第一行第一列的值就是判别为猫的概率，与第二列相乘，假设这是判别为狗的分类器，那么得到第一行第二列的值就是判别为狗的概率...以此类推。

  **非极大值抑制（NMS）**算法首先寻找得分最高的目标，然后计算其他目标与该目标的IoU值，最后删除所有IoU值大于给定阈值的目标。**最终目标**就是只留下一个评分最高的候选框。

  <img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324195401688.png" alt="image-20220324195401688" style="zoom: 33%;" />

  对概率矩阵的每一列进行非极大值抑制就能剔除很多重叠的建议框，从而保留高质量的建议框。

  

- 使用回归器精细修正候选框位置

  对NMS处理后剩余的建议框进一步筛选。接着分别用20个回归器对上述20个类别中剩余的建议框进行回归操作，最终得到每个类别的修正后的得分最高的bounding box。

  <img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324195838443.png" alt="image-20220324195838443" style="zoom:25%;" />

  如图黄色框表示建议框，绿色框表示实际框，红色窗口表示进行回归后的预测窗口。

### 存在的问题

1. 测试速度慢

   测试一张图片约53s（CPU），一张图像内候选框之间存在大量重叠，提取特征操作冗余。

2. 训练速度慢

   过程极其繁琐

3. 训练所需空间大

   对于SVM和bbox回归训练，需要从每个图像中的每个目标候选框提取特征，并写入磁盘。对于非常深的网络，如VGG16，从VOC07训练集上的5k图像上提取的特征需要数百GB的存储空间。

## Fast R-CNN

在同样使用VGG16作为网络的backbone时，与R-CNN相比训练时间快了9倍，测试推理时间快213倍，在Pascal VOC数据集上的准确率也从之前的62%提升至66%。

Fast R-CNN框架

![image-20220324205752924](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324205752924.png)

算法流程：

1. 一张图像生成1000~2000个**候选区域**（使用Selective Search方法）

2. 将图像输入到网络中得到相应的**特征图**，将SS算法生成的候选框投影到特征图上获得相应的**特征矩阵**

3. 将每个特征矩阵通过ROI（Region of Interest） pooling层缩放到$$7\times7$$大小的特征图，接着将特征图展平通过一系列全连接层得到预测结果

   <img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324201631553.png" alt="image-20220324201631553" style="zoom: 50%;" />

### 算法流程

**通过一次性计算整张图像特征**

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324202144177.png" alt="image-20220324202144177" style="zoom: 25%;" />

**训练数据的采样分为正样本和负样本**，正样本就是候选框中确实存在待检测目标，负样本则相反，候选框中不存在目标，可以简单理解成背景。在训练过程中并不是直接使用SS算法提供的所有候选区域，而是从中进行随机采样。

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324202710573.png" alt="image-20220324202710573" style="zoom:33%;" />

之后通过ROI Pooling层缩放到统一的尺寸，也就是$$7\times7$$大小的特征图

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324202836656.png" alt="image-20220324202836656" style="zoom:33%;" />

**通过分类器得到概率**

![image-20220324203107812](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324203107812.png)

**边界框回归器**，每个回归框都对应着4个参数

![image-20220324203309126](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324203309126.png)


$$
\begin{aligned}
&\hat{G}_{x}=P_{w} d_{x}(P)+P_{x} \\
&\hat{G}_{y}=P_{h} d_{y}(P)+P_{y} \\
&\hat{G}_{w}=P_{w} \exp \left(d_{w}(P)\right) \\
&\hat{G}_{h}=P_{h} \exp \left(d_{h}(P)\right)
\end{aligned}
$$
其中$$P_x,P_y,P_w,P_h$$分别为候选框的中心x，y坐标，以及宽高，得到的结果为最终预测的边界框中心x，y坐标，以及宽高

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324203953289.png" alt="image-20220324203953289" style="zoom:33%;" />

**损失计算**

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324204741283.png" alt="image-20220324204741283" style="zoom: 33%;" />

- p是分类器预测的softmax概率分布$$p=(p_0,...,p_k)$$

- u对应目标真实类别标签

- $$t^u$$对应边界框回归器预测的对应类别u的回归参数$$(t^u_x,t^u_y,t^u_w,t^u_h)$$

- v对应真实目标的边界框回归参数$$(v_x,v_y,v_w,v_h)$$

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324205143582.png" alt="image-20220324205143582" style="zoom: 33%;" />

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324205321016.png" alt="image-20220324205321016" style="zoom:33%;" />

## Faster R-CNN

网络结构

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324214516566.png" alt="image-20220324214516566" style="zoom: 33%;" />

同样使用VGG16作为网络的backbone，推理速度在GPU上达到5fps（包括候选区域的生成），准确率也有进一步的提升

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324210205152.png" alt="image-20220324210205152" style="zoom: 50%;" />

算法流程：

1. 将图像输入网络得到相应的特征图
2. 使用RPN结构生成候选框，将RPN生成的候选框投影到特征图上获得相应的特征矩阵
3. 将每个特征矩阵通过ROI Pooling层缩放到$$7\times7$$大小的特征图，接着将特征图展平通过一系列全连接层得到预测结果

### 传播过程

特点则是将之前的SS算法通过**RPN算法**进行替代，后面依旧使用Fast R-CNN算法。

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220407200452570.png" alt="image-20220407200452570" style="zoom: 50%;" />

在生成的特征图上使用一个滑动窗口在特征图上进行滑动，每滑动到一个位置就生成一个一维向量，在这个向量的基础上在通过两个全连接层分别输出目标概率和边界框回归参数

![image-20220324211228572](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324211228572.png)



<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324211405854.png" alt="image-20220324211405854" style="zoom:33%;" />

上面两个两个一组，下面四个四个一组。上面例如第一组0.1就代表它是背景的概率，0.9是检测目标的概率是0.9，这里只对它是前景或者背景进行判断，并不进行分类。

anchor一共有三种面积$${128^2,256^2,512^2}$$，三种比例$${1:1,1:2,2:1}$$，每个滑动窗口在原图上都对应有3x3=9anchor。如下图

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324212013834.png" alt="image-20220324212013834" style="zoom: 33%;" />



*通过一个小的感受野，去预测一个比他大的边界框是有可能的，根据经验所得，通过看到一个物体的一部分可以大概猜测出这个物体是什么*

对于一张$$1000\times600\times3$$的图像，大约有$$60\times 40\times 9(20k)$$个anchor，忽略跨越边界的anchor以后，剩下约6k个anchor。对于RPN生成的候选框之间存在大量重叠，基于候选框的cls得分，采用非极大值抑制，IoU设为0.7，这样每张图片只剩2k个候选框。

对于生成的上万的anchor中采样256个anchor，按正样本和负样本1：1选取，如果正样本不足128个，就用负样本进行填充。

**正样本的定义方式**：只要anchor与标注的目标边界框IoU超过0.7就定义为正样本；*找与标注的目标边界框相交最大的作为正样本*



损失计算

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220324213039976.png" alt="image-20220324213039976" style="zoom: 33%;" />

$$p_i$$表示第i个anchor预测为object的概率

$$p^*_i$$当为正样本时为1，当为负样本时为0

$$t_i$$表示预测第i个anchor的边界框回归参数

$$t*_i$$表示第i个anchor对应的GT Box

$$N_{cls}$$表示一个mini-batch中的所有样本数量256

$$N_{reg}$$表示anchor位置的个数约2400



### 模型训练

目前是采用RPN Loss+Fast R-CNN Loss的联合训练方法

原论文中采用分别训练RPN以及Fast R-CNN的方法

1. 利用ImageNet预训练分类模型初始化前置卷积网络层参数，并开始单独训练RPN网络参数
2. 固定RPN网络独有的卷积层以及全连接层参数，再利用ImageNet预训练分类模型初始化前置卷积网络参数，并利用RPN网络生成的目标建议框去训练Fast R-CNN网络参数
3. 固定利用Fast R-CNN训练好的前置卷积网络层参数，去微调RPN网络独有的卷积层以及全连接层参数
4. 同样保持固定前置卷积网络层参数，去微调Fast R-CNN网络的全连接层参数。最后RPN网络与Fast R-CNN网络共享前置卷积网络层参数，构成一个统一网络



![image-20220414225235875](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220414225235875.png)

在原本每个像素点9个anchor的基础上多增加了一种面积为32*32的anchor，也是分为1：1，0.5：2，2：0.5三种面积，但是实验结果表明，加上以后所有的精度全都下降了，包括大目标

![image-20220415075010946](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220415075010946.png)

## SSD

对于输入尺寸300x300的网络使用NvidiaTitanX在VOC2007测试集上达到74.3%mAP以及59FPS，对于512x512的网络，达到了76.9%mAP超越当时最强的FasterRCNN(73.2%mAP)。

Faster R-CNN存在的问题有：

1. 对小目标检测效果很差，这个模型只在一个特征层上进行预测，而这个feature maps是在经过很多个卷积层卷积后得到的，这里的feature maps以及被抽象到一个很高的层次，而抽象的程度越高，它的细节保留的也就越少，而对于小目标的检测就是需要这些细节信息。所以对于小目标的检测效果就不会很好
2. 模型太大导致检测速度较慢。

### 算法模型

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220325101350547.png" alt="image-20220325101350547" style="zoom:67%;" />

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220325101540721.png" alt="image-20220325101540721" style="zoom: 33%;" />

首先输入图像需要是300x300的，所以在将图片输入网络时需要进行将图片缩放到300x300，使用VGG网络Conv5_3也就是虚线之前的部分作为backbone，之后通过3x3的卷积层输出第一个特征层，之后通过一系列卷积层分别输出第2，3，4，5，6个特征层。使用浅层特征层来预测相对较小的目标，随着抽象程度不断加深会让它检测相对较大的目标。

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220325102544343.png" alt="image-20220325102544343" style="zoom:33%;" />

8x8的特征图会相比4x4的特征图保留更多的特征信息，那么就会在相对低层的特征图上对小目标进行预测。

![image-20220325103337232](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220325103337232.png)

上图为对于每个**特征层**的Default Box的scale以及aspect的设定

$$38\times 38\times 4+19\times 19\times 6+10\times 10\times 6+5\times 5\times 6+3\times 3\times 4+1\times 1\times 4=8732$$

![image-20220325104420914](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220325104420914.png)

**正样本**的选取

![image-20220325104855745](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220325104855745.png)

1. 对于每个ground truth box去匹配与它IoU值最大的default box
2. 对于每个default box，只要它与任意ground truth box的IoU大于0.5就选为正样本

**负样本**的选取

![image-20220325105239105](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220325105239105.png)

选取完正样本之后剩余的都是负样本，但都作为负样本会导致不平衡，所以选取负样本的策略如下：

1. 对剩下的负样本进行计算highest confidence loss，值越大表示网络会将这个负样本预测为目标的概率就越大，于是就把这些负样本进行选取。负样本与正样本的选取大概在3：1

**损失计算**

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220325105715918.png" alt="image-20220325105715918" style="zoom: 50%;" />

其中N为匹配到的正样本个数，α为1

![image-20220325105806540](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220325105806540.png)

$$c^p_i$$为预测的第i个default box对应GT box（类别是P）的类别概率

$$x^p_y={0,1}$$为第i个default box匹配到的第j个GT box（类别是P）

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220325110136672.png" alt="image-20220325110136672" style="zoom:50%;" />

$$l^m_i$$为预测对应第i个正样本回归参数

$$g^m_j$$为正样本i匹配的第j个GT box的回归参数



## YOLO系列

You Only Look Once

### YOLO v1

在2016 CVPR上发表，45FPS、448*448、63.4mAP。效果不如当时的SSD和Faster R-CNN.

### YOLO v3

结构图

![image-20220617005344957](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220617005344957.png)



主干特征提取网络：

输入：416，416，3

不断进行下采样

输出：

1. 52，52，256
2. 26，26，512
3. 13，13，1024

对于输出的第三层特征图进行5次卷积之后进行分类预测和回归预测

得到13,13,75->13,13,3(先验框),20(类别)+1(有目标？)+4(回归参数)	



## Transformer

与传统检测领域的区别

transformer就是把拿到的一个序列进行重新组合，文本数据本身就是一个词一个字的形式，本身就是一个序列，所以它理所当然可以这么做。而对于图像来说，如果能将这个图像数据转换为一个序列的形式，那么是不是就可以直接套用到transformer里去呢？

对于选取传统的CNN网络作为backbone时我们需要去考虑如何去设计卷积层的层数，结构怎么去设计。而使用transformer则可以直接省去这些步骤，轻松得到CNN很多层以后得到的结果。



transformer解决的问题就是重新组合各大输入向量，得到更完美的特征。

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220428214602199.png" alt="image-20220428214602199" style="zoom: 33%;" />

例如现有一组输入序列x1，x2，x3，x4，现在就要对这一组输入序列进行特征提取。

如图，这四个输入序列肯定是存在某些联系的，就比如文本中的词与词之间。每个序列中的每一个token都包含三个向量q,k,v.其中q就是代表查询向量，k代表被查询向量，v就是特征的表达。q1和k1，k2，k3，k4进行计算后，算完以后就能得到一组权重项，通过权重将特征v1，v2，v3，v4再进行组合。

### 视觉中的Attention

![image-20220428220012696](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220428220012696.png)

通过注意力机制，让计算机能够知道图片中哪些是前景，哪些是背景。这样在模型的训练过程中就能够多利用这些我们所需要关注的目标信息。

### ViT

ViT是**2020**年Google团队提出的将Transformer应用在**图像分类**的模型，虽然不是第一篇将transformer应用在视觉任务的论文，但是因为其模型“简单”且效果好，可扩展性强（scalable，模型越大效果越好），成为了transformer在CV领域应用的里程碑著作，也引爆了后续相关研究。

因为**transformer**和self-attention在NLP领域内效果很好，如何将其应用到CV中。

 一些工作会把CNN和自注意力混在一起用，另一些会把整个CNN都换成自注意力的方式。

做这些的原因其实都是直接将图片转换为序列会导致**序列过长**，比如一张图片224 *224，将其转为序列后，序列长度达到**50776**，复杂度相当于正常序列的100倍以上。

于是就有人提出将特征图作为输入而不是图片，以resnet50为例，他卷积到最后一层得到的特征图大小为14 *14.将其转换为序列时也就只有**196**的长度。

这篇论文中是将一张图片分成多个patch，将每个patch通过展平成序列后作为transformer的输入。类比一下就是一个句子有多少词相当于一张图片有多少patch。

在14M-300M images的数据集上做预训练以后，ViT在常规数据集上就能够得到跟现有残差网络相同或者更好的结果，甚至没有出现饱和现象。

同时，transformer，也给多模态的问题提出了一个很好的解决方案。

**整体架构**

![image-20220428221045136](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220428221045136.png)

$$
\begin{align}
&X=224\times224\times3 
\\&假设使用16\times16作为patch大小
\\&得到的(token)N=\frac{224^2}{16^2}=196也就是得到了196个图像块
\\&X=196\times768
\\&每个token的维度:16\times16\times3=768
\\&之后通过线性投射层，也就是一个全连接层E,E:768(前面算出来的维度)\times768(可以改变的参数D)
\\&经过全连接层后X\cdot E=196\times768，这样输入就变成了序列1d的token，不再是2d的图了
\\&再加上一个特殊的(cls)token:1\times768,把这个特殊的token与图像的token进行拼接
\\&得到最终整体输入到transformer当中的序列长度就是(196+1)\times768
\\&最后加上位置编码信息，编码信息就相当于一张表，每一行就代表一个序号，每一行是一个向量，向量的维度也是768(D),之后把这些位置信息，加到所有的token里面，进行SUM，所以最后序列还是197\times768
\\&
\\&
\end{align}
$$




举个例子。首先，将一张图片平均分成九份，得到9个窗口，假如每个窗口是10 * 10 * 3的大小，对于每个窗口都进行一个位置编码的操作（直接1-9编码，或者行列的形式编码）。之后将每个窗口都通过线性投射层转为一个向量，比如第一个窗口就转为1 * 300的向量，之后所有的向量都经过一个全连接层，进行维度转换，输出仍然也是向量。（0*是额外增加的一个token，目的是为了在不断地Encoder过程中整合后面1-9的所有特征，用来做最后的分类任务）

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220428223653477.png" alt="image-20220428223653477" style="zoom:33%;" />

MLP：全连接层

Norm：归一化层

Multi-Head Attention：多头注意力机制层

Embedded Patches：输入的序列

结构中同样采用了Resnet中提到的残差结构



### Swin Transformer

用了移动窗口的层级式的vision transformer。主要思想是想让vision transformer像CNN一样能够做层级式的特征提取， 从而导致提取的特征拥有多尺度的概念。

Swin transformer可以作为计算机视觉领域的一个通用的backbone来使用。

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220429114518780.png" alt="image-20220429114518780" style="zoom: 33%;" />

像ViT是把图片分成很多个patch，这样每个token自始至终代表的图片尺寸都是一样的，它通过全局的自注意力达到全局建模的能力，但是对于多尺度特征的把握就会弱一些。

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220429115240844.png" alt="image-20220429115240844" style="zoom:33%;" />

对于目标检测来说，运用最广的就是**FPN**思想。当有一个分层式的卷积神经网络时，每一层出来的特征所代表的感受野都是不一样的，能够抓住物体不同尺度的特征从而很好的处理物体不同尺寸的问题。

Swin Transformer采取在小窗口计算自注意力，而不是像ViT在整图上计算自注意力，这样在图片尺寸发生改变时计算复杂度相对整图计算会小很多。并且下采样率不是固定的16x，而是从4x到8x到16x，刚开始的patch大小是4 * 4的。通过合并小patch变成大patch，这样一个大的patch就能看到之前4个小patch所能看到的内容，感受野就增大了。这样也就得到了不同尺寸的特征图，这样就可以将其送入FPN做后续的检测任务。

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220429121126603.png" alt="image-20220429121126603" style="zoom:33%;" />



**移动窗口的操作**

在transformer的第l层把特征图分成小窗口就能有效降低序列长度从而减少计算复杂度，每个灰框小patch就是最基本的4 * 4的计算单元，每个红框就是一个窗口，自注意力的操作就是在这个红框内完成的。

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220429121630348.png" alt="image-20220429121630348" style="zoom:33%;" />

shift操作就是把整个窗口进行移动，在新的窗口位置再进行划分。这样窗口与窗口之间就可以互动了。原本情况一个窗口内的patch是永远无法注意到别的窗口内的信息，达不到使用transformer 的初衷，因为transformer的初衷就是更好的理解上下文。做完shift操作以后就可以达到原本的目的。

![image-20220429131026203](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220429131026203.png)



![image-20220429125934589](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220429125934589.png)
$$
\begin{align}
&X=224\times224\times3 
\\&使用4\times4作为patch大小
\\&得到的图片尺寸是56\times56\times48
\\&经过linear-Embedding层后，因为超参数C设置为96，所以输出为56\times56\times96->3136\times96,其中3136就是拉伸后的序列长度，96就是每个token的向量维度
\\&经过transformer之后输出仍然是56\times56\times96
\\&为了得到多尺度的特征信息，就需要一个层级式的transformer，需要像CNN中有一个池化操作，也就是patch-merging的操作
\end{align}
$$
![image-20220429125023737](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220429125023737.png)

**patch merging**:主要思想把临近的小patch合并成一个大的patch，达到下采样同样的效果

如图相当于进行一个二倍下采样的过程，一个张量分成四个张量，在C的维度上进行拼接。为了和VGG或者ResNet保持一致，最后通过一个1*1的卷积将通道数缩为2C。最终得到的结果就是空间大小减半，通道数加倍。



将Swin Transformer作为Faster-RCNN 的backbone，整个算法流程：

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220429132707416.png" alt="image-20220429132707416"  />

## ConvNeXt

ConvNext是在ResNet50模型的基础上，仿照Swin Transformer的结构进行改进而得到的纯卷积模型，原生模型是一个分类模型，但是其可以作为backbone被应用到任何其它模型中。



![preview](https://pic4.zhimg.com/v2-04dcd0ba943db17464da74eea4739e9b_r.jpg)







# 其他

## FPN

Feature Pyramid Networks,一种多尺寸，金字塔结构的深度学习网络。使用了FPN的Faster R-CNN，其检测结果在cocoAP上提升2.3个点，在pascalAP上提升3.8个点。其优势是在于对小尺寸目标的检测。

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220325114524605.png" alt="image-20220325114524605" style="zoom: 33%;" />

a：针对检测不同尺度的目标时将不同尺度的图片都通过算法进行预测，这样有多少个不同的尺度就要进行多少次预测，效率很低

b：Faster R-CNN所用的方式，将图片通过backbone得到最终的特征图，在最终的特征图上进行预测

c：SSD所用的方式，也是将图片通过backbone得到特征图，在正向传播过程中得到的不同特征图上分别进行预测

d：FPN结构，并不是在backbone的不同特征图上进行预测，而是将不同特征图上的特征进行融合，在融合后的特征图上再进行预测

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220325120212716.png" alt="image-20220325120212716" style="zoom:33%;" />

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220325120846103.png" alt="image-20220325120846103" style="zoom: 50%;" />

首先，这里不同的特征图是有要求的，必须要按2的整数倍进行选取。对于每个特征图都先使用一个1x1的卷积层进行卷积，得到的特征图的channel就都相等了，将高层次的特征图进行2倍的上采样，让他与下一层的宽和高都一样，这样就可以进行相加的操作了。

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220325121040174.png" alt="image-20220325121040174" style="zoom:50%;" />

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220325121834497.png" alt="image-20220325121834497" style="zoom:50%;" />

将ResNet50得到的特征图都都用1x1的卷积核进行卷积，然后不断进行2倍上采样进行相加得到不同的预测特征图。最终得到的P2就保留更多的细节信息，就可以用于检测$$32^2$$的小目标，P3用于检测$$64^2$$的目标



## 匈牙利匹配



# 目标检测流程

主要都分为三大块：数据预处理、网络搭建、损失函数

![image-20220421194804672](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220421194804672.png)

Faster-rcnn的基本结构。从图中可以看出整个网络分为3个部分：

1.Convolutional layers，通过一系列的conv+ReLU+pooling层来对输入的图像进行特征提取得到feature maps，该特征图被共享用于后续RPN层和全连接层

 

2.通过RPN(Region Proposal Networks)去生成候选区域，其中通过softmax来判断anchors属于positive还是negative，再利用bounding box regression 来修正anchors从而获得最终精确的proposals

 

3.Roi Pooling层结合1中得到的feature maps和2中得到的proposals，提取出proposal feature maps，输入到最后的全连接层中判定目标的类别。利用得到的proposal feature maps计算proposal的类别，同时再次利用bounding box regression来获得检测框的最终位置。

 

 

 

![image-20220421194819524](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220421194819524.png)

第一部分，conv layers

![image-20220421194826693](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220421194826693.png)

上图为VGG16的模型，这个模型中所有的卷积层都是kernel_size=3，padding=1，stride=1，所有的pooling层都是kernel_size=2，padding=0，stride=2。

对于输入的一幅任意大小的图像，将其缩放至224*224的大小，因为是彩色图像，所以存在R G B 三个分量，所以输入的数据其实是一个224*224*3的张量，接着通过两层3*3的卷积层，卷积核的个数为64个所以输出的channel为64，得到输出为224*224*64。接着通过一个2*2步距为2的最大下采样层，将高和宽缩减为原来的一半。然后通过两层3*3的卷积层，卷积核个数为256个，得到输出为112*112*128。再通过一个最大下采样层将高和宽缩减为原来的一半，之后通过三层3*3的卷积层，卷积核个数为256，得到输出为56*56*256，通过最大下采样层将宽高变为一半，再通过三层3*3的卷积层，卷积核个数为512，得到输出为28*28*512，通过最大下采样层将宽高置为一半，最后通过3层3*3的卷积层，卷积核个数为512，得到输出为14*14*512的特征矩阵

 

 

第二部分 RPN(Region Proposal Networks)

![image-20220421194851476](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220421194851476.png)

RPN分为两部分，上面一条通过softmax对anchors进行分类得到positive和negative分类；下面用于计算对于anchors的bounding box regression的偏移量从而获得精准的proposal；最后Proposal层综合positive anchors和对应的边界框回归的偏移量获取proposals，同时剔除太小和超出边界的proposals从而完成初步的目标定位

 ![image-20220421194903197](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220421194903197.png)

在之前得到的特征图上使用一个滑动窗口（3 * 3的卷积核，步距为 1，padding为1）在特征图上进行滑动，每滑动到一个位置就生成一个一维向量，在这个向量的基础上在通过两个全连接层分别输出目标概率和边界框回归参数。之前特征图的大小为14 * 14 * 512，所以这里每个向量就会有512个参数，有k个anchor box就会有2k个scores，因为对应每个anchor都会有两个概率，一个是为背景的概率，一个是为前景的概率。对于每个anchor也会生成4个边界框回归参数，所以是4k个。对于cls layer采用kernel size=1 * 1，卷积核个数为2k的卷积层进行处理，对于reg layer采用kernel size=1 * 1，卷积核个数为4k的卷积层进行处理。

![image-20220421194922444](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220421194922444.png)

![image-20220421194928224](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220421194928224.png)

比如第一组参数，0.1就代表他为前景的概率，0.9为是目标的概率。回归参数分别为预测坐标xy的偏移量，宽度和高度的偏移量。

Anchor box一共有三种面积128 ,256 , 512，三种比例1:1,1:2,2:1，每个滑动窗口在原图上都对应有3x3=9anchor.

![image-20220421194936169](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220421194936169.png)

在计算损失时 对于生成的anchor中采样256个anchor，按正样本和负样本1：1选取，如果正样本不足128个，就用负样本进行填充。

 

正样本的定义方式：只要anchor与标注的目标边界框IoU超过0.7就定义为正样本；找与标注的目标边界框相交最大的作为正样本

 

RPN最后输出的就是大量的proposal，也就是anchor的位置信息和类别信息，主要目的就是去除了大量的重复边界框

 

第三部分 Rol Pooling层及分类预测

之前是将anchor经过回归映射到原图上，现在再将其映射到特征图经过roi pooling层后输出的7*7*512大小的特征图上

![image-20220421194943044](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220421194943044.png)

将roi pooling层得到的7*7*512的特征图，先进行展平处理

![image-20220421194948208](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220421194948208.png)

![image-20220421194953612](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220421194953612.png)

这里的1024就是proposal的个数

![image-20220421194957984](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220421194957984.png)

通过并联两个全连接层，一个用来进行类别概率预测，一个用来进行边界框的回归预测

 

对于下面的类别预测器

![image-20220421195008491](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220421195008491.png)

20个类别加上一个背景类别，

对于边界框回归预测

![image-20220421195012791](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220421195012791.png)

 

最终得到预测的类别以及对应的坐标，选出每一行概率最大的类别当成置信度，根据置信度进行筛选排序，并且进行一次nms筛选，将IoU比较高的边框去掉，得到最终的预测结果

![image-20220421195033584](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220421195033584.png)



# 对于小目标解决方向

找到一个公共数据集，来更好的满足小目标的图片占比

- 在MS COCO数据集上，在训练集中41.4%为小目标，34.4%为中等目标，24.2%为大目标

- 只有一半的训练图片包含任何小目标，有70.07%的训练图片包含中等目标，82.28%训练图片包含大目标

- 候选框被标为正标签的条件是IoU>0.7,在这个前提下中目标和大目标明显会更容易获得高IoU的anchor box，小目标可能仅匹配到单个锚点框并且IoU很低。有必要强制给每个小目标的GT BOX匹配到一个锚点框，即使它的IoU很低很低，不然这个小目标永远都不会被训练到。（或者反向给小目标的GT BOX框的大一点？）

  ![image-20220415083656865](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220415083656865.png)

**过采样**

从数据预处理的角度去尝试解决，在训练过程中过采样包含小目标的图像，但这么做会导致中目标和大目标的训练也受到影响

![img](https://bbsmax.ikafan.com/static/L3Byb3h5L2h0dHBzL2ltZzIwMjAuY25ibG9ncy5jb20vYmxvZy8yMjU4MDk0LzIwMjEwNS8yMjU4MDk0LTIwMjEwNTE2MTEyMDE1MDA4LTEyODYzMDc1OTkucG5n.jpg)

**直接复制小目标**

对每个带有小目标的图像通过手动的方式进行目标复制，在不影响到别的目标的情况下，将小目标复制几份到别的空白位置上

![image-20220415093556358](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220415093556358.png)

![img](https://bbsmax.ikafan.com/static/L3Byb3h5L2h0dHBzL2ltZzIwMjAuY25ibG9ncy5jb20vYmxvZy8yMjU4MDk0LzIwMjEwNS8yMjU4MDk0LTIwMjEwNTE2MTEyMjM5MTMzLTE4NTg0MTAxNDQucG5n.jpg)

**数据增强**

超分辨率技术：

将低分辨率的图像变成高分辨率的图像，这就意味着能获得原始为小目标的目标更多的细节。

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220421210126107.png" alt="image-20220421210126107" style="zoom: 33%;" />

基于插值：

![image-20220421193750203](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220421193750203.png)

对一张低分辨率图将它的每个像素点临近插入空的像素点，从而得到高分辨率图。而插入的值则通过最近邻、双三次、双线性的方法进行计算。

基于深度学习的图像超分辨率重建：

<img src="C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220421211853849.png" alt="image-20220421211853849" style="zoom: 50%;" />

一张图片输入进来（HR），通过下采样的方式得到它的低分辨率图像（LR），之后将LR输入到模型中，得到超分辨率图像（SR），最后将SR和HR进行相似度的评估。

从流程上看，可以发现数据的获取非常简单，任意一张图像都能用来做训练。

**SRCNN**（Super Resolution Convolutional Neural Networks）：

![image-20220415115749833](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220415115749833.png)

![image-20220422134052088](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220422134052088.png)

![[公式]](https://www.zhihu.com/equation?tex=Conv%281%2C64%2C9%29-ReLU-Conv%2864%2C32%2C1%29-ReLU-Conv%2832%2C1%2C5%29)

作者这个思路是从稀疏编码得来的，并把上述过程分别表述为：Patch extraction, Non-linear mapping, Reconstruction。

- Patch extraction: 提取图像Patch，进行卷积提取特征，类似于稀疏编码中的将图像patch映射到低分辨率字典中

![img](https://pic2.zhimg.com/80/v2-77cc3d2dd3d7950dea3ac19fb52e5f19_1440w.jpg)

- Non-linear mapping: 将低分辨率的特征映射为高分辨率特征，类似于字典学习中的找到图像patch对应的高分辨字典

![img](https://pic3.zhimg.com/80/v2-e3e050d564b12f55453dec986ef12fc2_1440w.jpg)

- Reconstruction：根据高分辨率特征进行图像重建。类似于字典学习中的根据高分辨率字典进行图像重建

![img](https://pic3.zhimg.com/80/v2-886f3923fb3bbea0653dcd18c4187662_1440w.jpg)

算法流程：

输入图像img

设置缩放倍数up_scale

将图像img由rgb转成ycbcr格式

将图像img裁剪成up_scale整数倍的尺寸，得到图像im_g

对图像img_g进行bicubic操作得到图像img_l

对图像img_l进行SRCNN操作，得到图像img_h

分别对三个图像进行去边界操作

计算PSNR，输出最后的比对结果和生成的图像





输入一张低图像，通过双三次的插值将图像缩小再放大得到低分辨率图像，接着通过三层卷积网络拟合非线性映射，最后输出高分辨率图像结果。三个卷积层使用的卷积核的大小分为为9x9,1x1和5x5，前两个的输出特征个数分别为64和32。

评价指标

- **峰值信噪比** (PSNR)：直接对比预测结果与 GT 之间的像素差异
  - ![image-20220422134206163](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220422134206163.png)![image-20220422134236150](C:\Users\Aik\AppData\Roaming\Typora\typora-user-images\image-20220422134236150.png)
  - 均方误差（MSE）两个图像中每一个相同位置的[像素](https://so.csdn.net/so/search?q=像素&spm=1001.2101.3001.7020)值相减，平方，求和，再求平均。表达的是两幅图在每一个位置上的像素值的差异的平均。数值越大，表示两张图片在相似度上更低。
  - 
- 结构相似性 (SSIM)：对比预测结果与 GT 之间的结构相似程度
- 平均意见得分 (MOS)：人眼直观判断图像清晰度
- 感知质量 (PI)：评估图像的感知舒适度





在进行图像超分辨率时，图像被转化为 YCbCr 色彩空间，但是网络的训练过程中只会用到通道Y，然后，网络的输出合并已插值的 Cb Cr 通道，输出最终彩色图像。

RGB转换为YCbCr公式如下：

Y = 0.257*R + 0.504*G + 0.098*B + 16
Cb = -0.148*R - 0.291*G + 0.439*B + 128
Cr = 0.439*R - 0.368*G - 0.071*B + 128

**Y通道与灰度图像是不同**

RGB转换为灰度公式如下：

Gray = R*0.299 + G*0.587 + B*0.114



